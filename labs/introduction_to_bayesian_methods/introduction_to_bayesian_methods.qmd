---
title: "Introduction to Bayesian Methods"
subtitle: "ISEE Bayesian Modeling for Environmental Health Workshop"
author: "Robbie M. Parks"
date: "September 17 2023"
format: html
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(nimble)
library(bayesplot)
library(posterior)
library(hrbrthemes)

extrafont::loadfonts()
theme_set(theme_ipsum())

color_scheme_set(scheme = "viridis")
knitr::opts_chunk$set(eval = TRUE, echo = TRUE, warning = FALSE)
set.seed(2)
```

## Goal of this computing lab session

This goal of this lab is to take some models and concepts from the Introduction to Bayesian Methods lecture and introduce you to the way `NIMBLE` works.

## What's going to happen in this lab session?

During this lab session, we will:

1. Explore how `NIMBLE` is written and works;
2. Write some common regression models (Normal, Additional: logistic regression, Poisson);
3. Understand how basic model assessment is made; and
4. Test out how adjustments of models are made via different priors and more samples.

## Introduction

`NIMBLE` code is written in a slightly unusual format if you're used to just using base `R`.
It is written in the style of a program called BUGS, which came out a few decades ago and was developed at Imperial College London. But you don't really need to know about BUGS to be able to use `NIMBLE`.

We will start with some straightforward examples for situations you're likely familiar with to introduce the style of writing models. These examples will feature basic regression models using linear predictors.

::: aside
Some of these models have been adapted from [`NIMBLE`'s examples](https://r-nimble.org/examples).
:::

### Normal-Normal example
$$
\begin{split}
y_i &\sim \text{Normal}(\mu_i, \sigma) \quad i = 1,..., N \\
\mu_i &= \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\end{split}
$$

First create some example data:
```{r}
set.seed(1)

p <- 3 # number of explanatory variables
n <- 10000 # number of observations
X <- matrix(round(rnorm(p * n), 2), nrow = n, ncol = p) # explanatory variables
true_betas <- c(c(0.2, 0.5, 0.3)) # coefficients for beta1, beta2, beta3
sigma <- 0.5
y <- rnorm(n, X %*% true_betas, sigma)
```

## Exploratory data analysis

What does the dataset look like?
```{r}
df <- tibble(y = y, x1 = X[, 1], x2 = X[, 2], x3 = X[, 3])
df %>% head()
```

What does equivalent frequentist model output look like for reference?
By the way, what's the interpretation of the 95% CI in this frequentist world?
```{r}
model_freq <- lm(
  y ~ x1 + x2 + x3,
  data = df
)
central_est <- t(t(model_freq$coefficients))
conf_int <- confint(model_freq)
cbind(central_est, conf_int)
```

Let's rewrite this model in a Bayesian framework using `NIMBLE`. It can be written in lots of different ways, but here we're going to try and stick to defining the priors first, then writing the formula.
```{r}
code <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100) # prior for variance components

  # regression formula
  for (i in 1:n) {
    # n is the number of observations we have in the data
    mu[i] <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i] # manual entry of linear predictors
    y[i] ~ dnorm(mu[i], sd = sigma)
  }
})
```

Before running `NIMBLE`, extract data for three predictors and center for better MCMC performance.
```{r}
x1 <- X[, 1] - mean(X[, 1])
x2 <- X[, 2] - mean(X[, 2])
x3 <- X[, 3] - mean(X[, 3])
```

Final preparation of data into lists for input into `NIMBLE` MCMC running
```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```

Set initial values for MCMC samples (very important for convergence)
```{r}
inits <- list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0, sigma = 1)
```

The following code will establish which samples will be used in the sampling of the posteriors. It is not going to run the model yet.
If there is a conjugate relationship apparent between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma), `NIMBLE` will automatically detect it here. This is an example where there is a Normal-Normal conjugate relationship
```{r}
model <- nimbleModel(code, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model)
```

Run the MCMC simulations. There are lots of arguments in the main function, nimbleMCMC(). Take a second to look through each of them and make sure you understand where they're coming from and what they mean as each one is very important.
```{r}
tic <- Sys.time()
nimbleMCMC_samples_initial <- nimbleMCMC(
  code = code,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000, # run 10000 samples
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc <- Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples? The following operations will help us understand.

First a summary of how the statistics of the samples of each parameter look
```{r}
summarise_draws(nimbleMCMC_samples_initial, default_summary_measures())
```

Then examine how well the model has converged, which typically is identified by how close each rhat value is to 1.00. If it is much above 1.00 (>1.05) then the samples will not be very well converged and therefore either the model is not well specified or there just needs to be more iterations of the MCMC. It is a little bit of an art to figure that out, but rhat helps.
```{r}
summarise_draws(nimbleMCMC_samples_initial, default_convergence_measures())
```

What do the samples of one of the unknown parameters actually look like?
Let's have a look the parameters and their samples from running the model above.
```{r}
mcmc_trace(nimbleMCMC_samples_initial)
```

Let's now focus on `beta1` (which we know is 0.2 from setting up initially).
```{r}
mcmc_trace(nimbleMCMC_samples_initial, pars = c("beta1"))
```

It looks like the samples are converging quickly from the initial parameter to ~0.2 for `beta1`. But typically we will throw away some samples at the beginning to ensure that the transient samples (which is when the model samples haven't stabilized around a particular value because they are travelling from the initial values we give them as above for `inits`) are not included in calculating estimates of the mean and credible intervals, which could bias the results up or down slightly.

For example, you can see how the `sigma` parameter had to jump from it's initial value of 1.0 before stabilizing around its true value of 0.5.

This period where we throw away samples (defined by the `nburnin` argument below) is called the 'burn in' or 'warm up' period.

Let's do it again but with a burn in of 1000 samples, i.e., throw away the first 1000 samples because they may not yet be centered around a particular stable value (assuming the model is reasonably parameterized, this will happen at some point).
```{r}
tic <- Sys.time()
nimbleMCMC_samples_burnin <- nimbleMCMC(
  code = code,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000, # collect 10000 samples
  nburnin = 1000, # burn in for 1000 iterations
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc <- Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples with burn in?
```{r}
summarise_draws(nimbleMCMC_samples_burnin, default_summary_measures())
```

And how good do convergence indicators look?
```{r}
summarise_draws(nimbleMCMC_samples_burnin, default_convergence_measures())
```

Now the samples for each parameter look to be very tidily centred around their estimated values, with `sigma` now not showing that unusual journey from its initial starting value as the burn in period has thrown that out.
```{r}
mcmc_trace(nimbleMCMC_samples_burnin)
```

Let's focus on the `sigma` plot
```{r}
mcmc_trace(nimbleMCMC_samples_burnin, pars = c("sigma"))
```

Let's try changing the priors and see if that makes any difference to how the model fits. Take some to compare this with the code for the original model above. Notice how the real value of `beta2` (0.5) is not allowed as a value from the prior it is assigned below.
```{r}
code_worse_priors <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dunif(100, 100000) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100) # prior for variance components

  # regression formula
  for (i in 1:n) {
    mu[i] <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i] # manual entry of linear predictors
    y[i] ~ dnorm(mu[i], sd = sigma)
  }
})
```

Run the MCMC simulations
```{r}
tic <- Sys.time()
nimbleMCMC_samples_worse_priors <- nimbleMCMC(
  code = code_worse_priors,
  data = data,
  constants = constants,
  inits = list(alpha = 0, beta1 = 0, beta2 = 1000, beta3 = 0, sigma = 1),
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc <- Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples with different priors?
```{r}
summarise_draws(nimbleMCMC_samples_worse_priors, default_summary_measures())
```

And how good do convergence indicators look?
```{r}
summarise_draws(nimbleMCMC_samples_worse_priors, default_convergence_measures())
```

What do the samples look like here?
```{r}
mcmc_trace(nimbleMCMC_samples_worse_priors)
```

You can see how the model cannot recover the true value of `beta2` because of our choice of prior.

By using a uniform prior without support for the true value of `beta2`, the model has increased its variance to try and cope with the data.

This is a warning: even though the mode has converged, it might not be a good model.

Back to our sensible priors, say we wanted to establish the estimated difference between two betas?
Let's say `beta1` and `beta2` in this case. `NIMBLE` formulas are flexible so you can define this as an output of the model, as seen in last elements of formula code below.
```{r}
code_diff_betas <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3
  sigma ~ dunif(0, 100) # prior for variance components

  # regression formula
  for (i in 1:n) {
    mu[i] <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i]
    y[i] ~ dnorm(mu[i], sd = sigma)
  }

  # difference between beta1 and beta2
  beta12_diff <- beta2 - beta1
})
```

Run the MCMC simulations and specifically only monitor differences between `beta1` and `beta2` to get an estimate of the difference between `beta1` and `beta2`.
```{r}
parameters_to_monitor <- c("beta12_diff")

tic <- Sys.time()
nimbleMCMC_samples_beta12_diff <- nimbleMCMC(
  code = code_diff_betas,
  data = data,
  constants = constants,
  inits = inits,
  monitors = parameters_to_monitor,
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc <- Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the samples when monitoring difference between `beta1` and `beta2`?
```{r}
summarise_draws(nimbleMCMC_samples_beta12_diff, default_summary_measures())
```

And how good do convergence indicators look?
```{r}
summarise_draws(nimbleMCMC_samples_beta12_diff, default_convergence_measures())
```

Equally, we could also do this by operating on the samples themselves from the original model. `NIMBLE` is quite flexible.
```{r}
(nimbleMCMC_samples_burnin[, "beta2"] - nimbleMCMC_samples_burnin[, "beta1"]) |>
  as.numeric() |>
  summary()
```

### Logistic regression (binomial) example.

Logistic regression is used when we want to classify observations into two groups.
$$
\begin{split}
y_i &\sim \text{Bernoulli}(p_i) \quad i = 1,..., N \\
\text{logit}(p_i) &= \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\end{split}
$$

::: aside
Note, a Bernoulli likelihood is equivalent to a Binomial with sample size 1.
:::

First create some example data for our model (for success (1) or failure (0) as the outcome):
```{r}
n <- 10000
p <- 3

set.seed(1)
x1 <- round(rnorm(n), 2)
x2 <- round(rnorm(n), 2)
x3 <- round(rnorm(n), 2)

z <- 1 + 2 * x1 + 3 * x2 - 5 * x3 # linear combination with a bias of 1
pr <- 1 / (1 + exp(-z)) # pass through an inv-logit function
y <- rbinom(n, 1, pr) # bernoulli response variable
```

What does the dataset look like?
```{r}
df <- tibble(y = y, x1 = x1, x2 = x2, x3 = x3)
df
```

What does equivalent frequentist model output look like for reference?
```{r}
# now feed it to glm:
model_freq <- glm(
  y ~ x1 + x2 + x3,
  family = "binomial",
  data = df
)

central_est <- t(t(model_freq$coefficients))
conf_int <- confint(model_freq)
cbind(central_est, conf_int)
```

Create the `NIMBLE` model for a logistic regression.
```{r}
code_binomial <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3

  # regression formula
  for (i in 1:n) {
    y[i] ~ dbin(p[i], 1)
    logit(p[i]) <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i]
  }
})
```

Before running `NIMBLE`, extract data for three predictors and center around zero for better MCMC performance.
```{r}
x1 <- x1 - mean(x1)
x2 <- x2 - mean(x2)
x3 <- x3 - mean(x3)
```

Final preparation of data into lists.
```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```

Set initial values for MCMC samples.
```{r}
inits <- list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0)
```

Let `NIMBLE` find out if there is a conjugate relationship between prior and posterior (e.g., Normal-Normal, Binomial-Beta, Poisson-Gamma).
```{r}
model_binomial <- nimbleModel(code_binomial, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model_binomial)
```

Let's run the model. We're going to go with the burn in period from the start here to throw away the weird transitionary samples.
```{r}
tic <- Sys.time()

nimbleMCMC_samples_binomial <- nimbleMCMC(
  code = code_binomial,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc <- Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the binomial model?
```{r}
summarise_draws(nimbleMCMC_samples_binomial, default_summary_measures())
```

And how good do convergence indicators look?
```{r}
summarise_draws(nimbleMCMC_samples_binomial, default_convergence_measures())
```

Now the samples, taking note, e.g., that the samples for `beta1` look to be very tidily centered around 2.
```{r}
mcmc_trace(nimbleMCMC_samples_binomial)
```

### Poisson example.

$$
\begin{split}
y_i &\sim \text{Pois}(\mu_i) \quad i = 1,..., N \\
\log(\mu_i) &= \alpha + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3
\end{split}
$$

Again, let's make some simulated data but this time for a Poisson model (for count data)
```{r}
n <- 10000

set.seed(1)
x1 <- round(rnorm(n), 2)
x2 <- round(rnorm(n), 2)
x3 <- round(rnorm(n), 2)

z <- 4 + 0.3 * x1 - 0.1 * x2 + 0.6 * x3
lambda <- exp(z)
y <- rpois(n, lambda)
```

What does the dataset look like?
```{r}
df <- tibble(y = y, x1 = x1, x2 = x2, x3 = x3)
df
```

What does equivalent frequentist model output look like for reference? What's the interpretation of the 95% CI?
```{r}
# now feed it to glm:
model_freq <- glm(
  y ~ x1 + x2 + x3,
  family = "poisson",
  data = df
)

central_est <- t(t(model_freq$coefficients))
conf_int <- confint(model_freq)
cbind(central_est, conf_int)
```

Create the `NIMBLE` model for the Poisson model.
```{r}
code_poisson <- nimbleCode({
  # priors for parameters
  alpha ~ dnorm(0, sd = 100) # prior for alpha
  beta1 ~ dnorm(0, sd = 100) # prior for beta1
  beta2 ~ dnorm(0, sd = 100) # prior for beta2
  beta3 ~ dnorm(0, sd = 100) # prior for beta3

  # regression formula
  for (i in 1:n) {
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- alpha + beta1 * x1[i] + beta2 * x2[i] + beta3 * x3[i]
  }
})
```

Before running `NIMBLE`, extract data for three predictors and center around zero for better MCMC performance
```{r}
x1 <- x1 - mean(x1)
x2 <- x2 - mean(x2)
x3 <- x3 - mean(x3)
```

Final preparation of data into lists
```{r}
constants <- list(n = n)
data <- list(y = y, x1 = x1, x2 = x2, x3 = x3)
```

Set initial values for MCMC samples
```{r}
inits <- list(alpha = 0, beta1 = 0, beta2 = 0, beta3 = 0)
```

Are there any conjugate relationships?
```{r}
model_poisson <- nimbleModel(code_poisson, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model_poisson)
```

Let's run the model.
```{r}
tic <- Sys.time()
nimbleMCMC_samples_poisson <- nimbleMCMC(
  code = code_poisson,
  data = data,
  constants = constants,
  inits = inits,
  niter = 10000,
  nburnin = 1000,
  setSeed = 1,
  samplesAsCodaMCMC = TRUE
)

toc <- Sys.time()
toc - tic
```

What is the summary of each estimated parameter from the Poisson model?
```{r}
summarise_draws(nimbleMCMC_samples_poisson, default_summary_measures())
```

And how good do convergence indicators look?
```{r}
summarise_draws(nimbleMCMC_samples_poisson, default_convergence_measures())
```

Now the samples for, e.g., `beta1` look to be very tidily centred around 0.3.
```{r}
mcmc_trace(nimbleMCMC_samples_poisson)
```

## Closing remarks

In this lab session, we have explored how to fit some basic models using Bayesian regression in `NIMBLE`.
We looked at the three most common likelihoods: Normal, Binomial, and Poisson.

We used simulated data here, so we had complete control over the complexity in the data.
However, in the real world, we would often try many different models if we do not know how the data were generated.
Also, not all real life examples fit the any of the three likelihoods perfectly.
For example, the data might be "overdispersed", where there is greater variability in the data than would be expected.
Luckily, there are extensions to these likelihoods that can deal with overdispersion, such as the beta-binomial likelhood for overdispersed binomial data, or the negative binomial likelihood (sometimes called the gamma-Poisson) for overdispersed Poisson data.
For more information on these likelihoods, see Chapter 12 of _Statistical Rethinking_ by Richard McElreath.
